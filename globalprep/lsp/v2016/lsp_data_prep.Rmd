---
title: 'OHI: Lasting Special Places '
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    toc: true
    number_sections: true
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '~/github/ohiprep/src/templates/ohi_hdr.html'
  pdf_document:
    toc: true
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}

knitr::opts_chunk$set(fig.width = 6, fig.height = 4, fig.path = 'Figs/',
                      echo = FALSE, message = FALSE, warning = FALSE)

# library(foreign)
# library(sp)
# library(rgdal)
# library(raster)
# library(maptools)
library(readr)

source('~/github/ohiprep/src/R/common.R')

goal     <- 'lsp'
scenario <- 'v2016'
dir_anx       <- file.path(dir_M, 'git-annex/globalprep') 
dir_goal      <- file.path('~/github/ohiprep/globalprep', goal, scenario)
dir_goal_anx  <- file.path(dir_anx,            goal, scenario)
dir_data_wdpa <- file.path(dir_anx, '_raw_data/wdpa_mpa', 'd2016') 

### set up provenance tracking for this script:
# source(file.path('~/github/ohibc', 'src/R/prov.R'))          

if(!file.exists(file.path(dir_goal, 'README.md'))) {
  warning(sprintf('No README detected in %s', dir_goal))
}


```

### Filter and re-project WDPA polygons

The WDPA-MPA dataset comes as a shapefile or geodatabase in WGS84 coordinate reference system.  

* For OHI we have chosen to count only protected areas with defined legal protection, so we apply a filter on the STATUS attribute that selects only STATUS == "Designated". 
  * According to the WDPA Manual:  STATUS as "Designated" means:  "Is recognized or dedicated through legal means. Implies specific binding commitment to conservation in the long term. Applicable to government and non-government sources."
  * Other values for STATUS include "Proposed", "Adopted", "Inscribed", or "Not Reported".
    * "Adopted" and "Inscribed" are World Heritage or Barcelona Convention sites; while these may seem important, they are generally protected by other means (as overlapping "Designated" polygons) in addition to these values.
* In 2015, the USA started including polygons that represent marine management plans, in addition to more strictly defined protected areas.  This info is contained in the "MANG_PLAN" field.
  * These programmatic management plans variously protect species, habitats, and (??) and can be MPA or non-MPA.
  * For OHI we have chosen to count only MPA programmatic management plans, omitting Non-MPA programmatic management plans.
* For ease of tallying areas, we convert the polygons to a Mollweide equal-area projection before rasterizing.

``` {r wdpa_shapefile1}

shp_raw     <- file.path(dir_data_wdpa, 'WDPA_May2016-shapefile', 'WDPA_May2016-shapefile-polygons')
shp_reorder <- file.path(dir_data_wdpa, 'shps', 'WDPA_May2016_shp_ordered')
shp_xformed <- file.path(dir_data_wdpa, 'shps', 'WDPA_May2016_shp_xformed')

if(!file.exists(paste0(shp_xformed, '.shp'))) {
  message('No shp found for filtered/reordered/transformed WDPA database')
  if(!file.exists(paste0(shp_reorder, '.shp'))) {
    message('No shp found for filtered/reordered WDPA database')
    ### Read in the raw shapefile (3.2 GB)
    message('Reading in raw shapefile: \n  ', shp_raw)
    ptm <- proc.time()
    wdpa_poly <- readOGR(dsn = dirname(shp_raw), layer = basename(shp_raw))
    message('elapsed: ', (proc.time() - ptm)[3])
    
    ### filter polygons
    wdpa_poly <- wdpa_poly[wdpa_poly@data$STATUS == 'Designated', ]
    wdpa_poly <- wdpa_poly[!str_detect(tolower(wdpa_poly@data$MANG_PLAN), 'non-mpa program'), ]
    
    ### reorder polygons (oldest last) and save shapefile
    reorder_vec <- order(wdpa_poly@data$STATUS_YR, decreasing = TRUE)
    wdpa_poly1 <- wdpa_poly[reorder_vec, ]
    
    message('Writing filtered/reordered WDPA polygons to: \n  ', shp_reorder)
    writeOGR(wdpa_poly1, dsn = dirname(shp_reorder), layer = basename(shp_reorder),
             driver = 'ESRI Shapefile')
    ### lots of warnings similar to:
    ###   "Warning 1: Value 555593627 of field WDPAID of feature 507 not successfully written."
    ### warning ignored: WDPAID field not used in analysis
    
    rm('wdpa_poly') ### clean up memory
 
  } else {
    ### reordered shapefile exists; read it in
    message('Reading in filtered/re-ordered shapefile: \n  ', shp_reorder)
    wdpa_poly1 <- readOGR(dsn = dirname(shp_reorder), layer = basename(shp_reorder))
  }
  
  message('Spatial transforming WDPA polygons to Mollweide')
  crs_mol <- CRS('+proj=moll +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs')
  ptm <- proc.time()
  wdpa_poly2 <- spTransform(wdpa_poly1, crs_mol)
  message('elapsed: ', (proc.time() - ptm)[3])  ### elapsed: 485.793s on Mazu

  message('Writing filtered/reorderedtransformed WDPA polygons to: \n  ', shp_xformed)
  ptm <- proc.time()
  writeOGR(wdpa_poly2, dsn = dirname(shp_xformed), layer = basename(shp_xformed),
           driver = 'ESRI Shapefile')
  message('elapsed: ', (proc.time() - ptm)[3])
  ### elapsed: 1445.093 s on Mazu

  rm(c('wdpa_poly1', 'wdpa_poly2')) ### clean up the memory

} else {
  ### transformed shapefile exists
  message('Transformed WDPA shapefile exists at \n  ', shp_xformed)
  message('Go inspect it to make sure it looks good!')
}

```


``` {r check_data_attributes_for_probs, eval = FALSE}
### to check existing SPDF for unfiltered polygons:
wdpa_df <- foreign::read.dbf(paste0(shp_xformed, '.dbf'))

names(wdpa_df)
#  WDPAID | WDPA_PID | PA_DEF | NAME | ORIG_NAME | DESIG | DESIG_ENG | DESIG_TYPE | IUCN_CAT
#  INT_CRIT | MARINE | REP_M_AREA | GIS_M_AREA | REP_AREA | GIS_AREA | NO_TAKE | NO_TK_AREA | STATUS
#  STATUS_YR | GOV_TYPE | OWN_TYPE | MANG_AUTH | MANG_PLAN | VERIF | METADATAID | SUB_LOC | PARENT_ISO
#  ISO3
nonmpa_check <- wdpa_df %>%
  filter(ISO3 == 'USA') %>% ### USA non-programmatic management plans were a problem in 2015
  select(MANG_PLAN) %>%
  distinct()
### if there are any non-MPA programmatic species management plans in here, filter 'em

newstatus_check <- wdpa_df %>%
  filter(STATUS %in% c("Adopted", "Inscribed"))
doubled_check <- wdpa_df %>%
  filter(str_detect(tolower(NAME), "yosemite|yellowst|chitwan|tikal|nahanni"))
### looks like most of those parks are covered under other rubrics as well

### check distribution of STATUS_YR to see if there are breaks 
status_yr <- wdpa_df$STATUS_YR
sum(status_yr == 0) ### 22072 (about 10%)
hist(status_yr[status_yr > 0])

quantile(status_yr, probs = seq(0, 1, 0.1))
```

### Rasterize WDPA polygons

Once reprojected to Mollweide, the WDPA polygons are rasterized to a 500 m grid.  This resolution is necessary to reasonably capture the "signal" of a 1000 m feature, i.e. the 1 km inland coastal zone.  Due to the fine scale, we continue to rely on ArcGIS functionality to perform this rasterization.  For future years we hope to perform this in an open-source system such as GDAL.

* The `rasterize_wdpa.py` script is an ArcPy script, and therefore, unfortunately, needs to be run on a Windows system with access to ArcGIS. 
* In addition to the filtered and reprojected WDPA polygons, the `rasterize_wdpa.py` script relies on an existing 500 m Mollweide raster to set extents and resolution.  Either of these rasters (for the 3 nautical mile offshore and 1 km inland regions) can be used:
    * `git-annex/globalprep/spatial/v2015/data/rgn_raster_500m/rgn_inland1km_mol_500mcell.tif`
    * `git-annex/globalprep/spatial/v2015/data/rgn_raster_500m/rgn_offshore3nm_mol_500mcell.tif`
* The output from `rasterize_wdpa.py` is saved as a 500 m global Mollweide raster, in which each cell value indicates the *earliest* year of protection.
    * `git-annex/globalprep/lsp/v2016/int/wdpa_designated_mol.tif`
    
``` {r show_global_wdpa_raster}

rast_wdpa_file <- file.path(dir_goal_anx, 'int/wdpa_designated_mol.tif')
rast_wdpa <- raster::raster(rast_wdpa_file)

ext <- raster::extent(c('xmin' = -12e6, 'xmax' = -8e6, 'ymin' = 2e6, 'ymax' = 5e6))
rast_wdpa_crop <- raster::crop(rast_wdpa, ext)


```


#### Global WDPA raster

`r plot(rast_wdpa)`

#### WDPA raster, Mexico and southern US

`r plot(rast_wdpa_crop)`


### Zonal statistics

Comparing the global WDPA raster to the 3 nm offshore and 1 km inland rasters, we can tally the protected area within each region and compare to the total area within each region.  Note each cell is 500 m^2^, so area is .25 km^2^, but since we are simply calculating a ratio, this cancels out.

``` {r lsp_zonal_stats}

zonal_3nm_file <- file.path(dir_goal, 'int', 'zonal_stats_3nm.csv')
zonal_1km_file <- file.path(dir_goal, 'int', 'zonal_stats_1km.csv')

if(!file.exists(zonal_3nm_file) | !file.exists(zonal_1km_file)) {
  
  ### point to 500 m rasters for 3 nautical mile coastal regions, and 1 km inland coastal regions.
  rast_3nm_file <- file.path(dir_anx, 'spatial/v2015/data/rgn_raster_500m/rgn_offshore3nm_mol_500mcell.tif')
  rast_1km_file <- file.path(dir_anx, 'spatial/v2015/data/rgn_raster_500m/rgn_inland1km_mol_500mcell.tif')
  
  rast_3nm <- raster::raster(rast_3nm_file)
  rast_1km <- raster::raster(rast_1km_file)
  
  ### NOTE: The crosstab function returns this warning - does it affect the
  ### outcomes, or does the function coerce the correct outcome?
      # Warning message:
      # In FUN(X[[i]], ...) : integer overflow - use sum(as.numeric(.))
  
  ptm <- proc.time()
  stats_3nm <- raster::crosstab(rast_wdpa, rast_3nm, useNA = TRUE, progress = 'text') %>%
    as.data.frame()
    setNames(c('year', 'rgn_id', 'n_cells')) %>%
    mutate(year   = as.integer(as.character(year)),
           rgn_id = as.integer(as.character(rgn_id))) %>%
    arrange(rgn_id, year)
  
  stats_1km <- raster::crosstab(rast_wdpa, rast_1km, useNA = TRUE, progress = 'text') %>%
    as.data.frame() %>%
    setNames(c('year', 'rgn_id', 'n_cells')) %>%
    mutate(year   = as.integer(as.character(year)),
           rgn_id = as.integer(as.character(rgn_id))) %>%
    arrange(rgn_id, year)
  
  message('Elapsed: ', (proc.time() - ptm)[3], ' sec')
  
  
  write_csv(stats_3nm, zonal_3nm_file)
  write_csv(stats_1km, zonal_1km_file)
} else {
  message('Zonal stats layers already exist: \n  ', zonal_3nm_file, '\n  ', zonal_1km_file)
}

```

``` {r summarize_zonal_stats}

stats_3nm <- read_csv(file.path(dir_goal, 'int', 'zonal_stats_3nm.csv'))
stats_1km <- read_csv(file.path(dir_goal, 'int', 'zonal_stats_1km.csv'))

rgn_area_1km <- read_csv(file.path('~/github/ohi-global/eez2013/layers', 'rgn_area_inland1km.csv'))
rgn_area_3nm <- read_csv(file.path('~/github/ohi-global/eez2013/layers', 'rgn_area_offshore3nm.csv'))

lsp_thresh <- 0.30
### Determine total cells per region (n_cells_tot) and then a cumulative
### total of cells per region
prot_1km <- stats_1km %>%
  group_by(rgn_id) %>%
  mutate(n_cells_tot = sum(n_cells),
         n_cells_cum = cumsum(n_cells),
         a_tot_cells = n_cells_tot / 4,
         a_prot_cells = n_cells_cum / 4) %>%
  ungroup() %>%
  left_join(rgn_area_1km %>% rename(a_tot_rgn = area_km2), by = 'rgn_id') %>%
  filter(!is.na(year))  %>% ### this ditches non-protected cell counts but already counted in n_cells_tot
  mutate(pct_prot_cells   = n_cells_cum / n_cells_tot,
         lsp_status_cells = ifelse(pct_prot_cells > lsp_thresh, 100, (pct_prot_cells / lsp_thresh) * 100)) %>%
  mutate(pct_prot_rgn     = a_prot_cells / a_tot_rgn,
         lsp_status_rgn   = ifelse(pct_prot_rgn > lsp_thresh, 100, (pct_prot_rgn / lsp_thresh) * 100))

prot_3nm <- stats_3nm %>%
  group_by(rgn_id) %>%
  mutate(n_cells_tot = sum(n_cells),
         n_cells_cum = cumsum(n_cells),
         a_tot_cells = n_cells_tot / 4,
         a_prot_cells = n_cells_cum / 4) %>%
  ungroup() %>%
  left_join(rgn_area_3nm %>% rename(a_tot_rgn = area_km2), by = 'rgn_id') %>%
  filter(!is.na(year))  %>% ### this ditches non-protected cell counts but already counted in n_cells_tot
  mutate(pct_prot_cells   = n_cells_cum / n_cells_tot,
         lsp_status_cells = ifelse(pct_prot_cells > lsp_thresh, 100, (pct_prot_cells / lsp_thresh) * 100)) %>%
  mutate(pct_prot_rgn     = a_prot_cells / a_tot_rgn,
         lsp_status_rgn   = ifelse(pct_prot_rgn > lsp_thresh,   100, (pct_prot_rgn / lsp_thresh) * 100))

write_csv(prot_3nm, file.path(dir_goal, 'int', 'area_protected_3nm.csv'))
write_csv(prot_1km, file.path(dir_goal, 'int', 'area_protected_1km.csv'))

```


``` {r combine_inland_and_offshore}

prot_df <- prot_1km %>%
  dplyr::select(rgn_id, year, 
                lsp_st_1km_rgn   = lsp_status_rgn, 
                lsp_st_1km_cells = lsp_status_cells,
                a_tot_rgn_1km    = a_tot_rgn,
                a_tot_cells_1km  = a_tot_cells) %>%
  left_join(prot_3nm %>% 
              dplyr::select(rgn_id, year, 
                            lsp_st_3nm_rgn   = lsp_status_rgn, 
                            lsp_st_3nm_cells = lsp_status_cells,
                            a_tot_rgn_3nm    = a_tot_rgn,
                            a_tot_cells_3nm  = a_tot_cells),
            by = c('rgn_id', 'year')) %>%
  mutate(lsp_status_rgn = (lsp_st_1km_rgn + lsp_st_3nm_rgn) / 2,
         lsp_status_cells = (lsp_st_1km_cells + lsp_st_3nm_cells) / 2)

rgn_names <- foreign::read.dbf(file.path(dir_anx, 'spatial/v2015/data/regions_gcs.dbf')) %>%
  filter(rgn_typ == 'eez') %>%
  select(rgn_id, rgn_name = rgn_nam)




rgn_poly_gcs <- rgdal::readOGR(dsn = file.path(dir_anx, '../Global/NCEAS-Regions_v2014/data'), 
                               layer = 'rgn_inland1km_gcs', 
                               stringsAsFactors = FALSE)
rgn_poly_mol <- rgdal::readOGR(dsn = file.path(dir_anx, '../Global/NCEAS-Regions_v2014/data'), 
                               layer = 'rgn_inland1km_mol', 
                               stringsAsFactors = FALSE)
rgns_to_check <- c(65, 196, 150, 197, 72, 199, 194, 66)

rgn_1km_gcs <- rgn_poly_gcs[rgn_poly_gcs@data$rgn_id %in% rgns_to_check, ]
rgn_1km_mol <- rgn_poly_mol[rgn_poly_mol@data$rgn_id %in% rgns_to_check, ]

rgn_1km_gcs_areas <- rgeos::gArea(rgn_1km_gcs, byid = TRUE)
rgn_1km_mol_areas <- rgeos::gArea(rgn_1km_mol, byid = TRUE)

area_mol_df <- rgn_poly_mol@data %>%
  mutate(area_recalc = rgeos::gArea(rgn_poly_mol, byid = TRUE))

area_mol_df <- area_mol_df %>%
  mutate(area_recalc = area_recalc/1e6) %>%
  rename(area_km2_from_shp = area_km2,
         area_km2_from_gArea = area_recalc) %>%
  left_join(rgn_area_1km %>% rename(area_km2_from_layer = area_km2), by = 'rgn_id')

area_mol_df <- area_mol_df %>%
  select(rgn_id, rgn_name, area_km2_from_shp, Shape_Area, area_km2_from_gArea, area_km2_from_layer)
write_csv(area_mol_df, file.path(dir_goal, 'int/area_check.csv'))


area_df <- prot_df %>%
  dplyr::select(rgn_id, a_tot_rgn_1km, a_tot_rgn_3nm, a_tot_cells_1km, a_tot_cells_3nm) %>%
  distinct() %>%
  mutate(a_1km_pct_diff = (a_tot_rgn_1km - a_tot_cells_1km) / a_tot_rgn_1km,
         a_3nm_pct_diff = (a_tot_rgn_3nm - a_tot_cells_3nm) / a_tot_rgn_3nm) %>%
  left_join(rgn_names, by = 'rgn_id')

write_csv(area_df, file.path(dir_goal_anx, 'int/area_check.csv'))

rgn_names <- foreign::read.dbf(file.path(dir_anx, 'spatial/v2015/data/regions_gcs.dbf')) %>%
  filter(rgn_typ == 'eez') %>%
  select(rgn_id, rgn_name = rgn_nam)

### check vs 2015 assessment (using eez2013 scores)
lsp_v2015 <- read_csv('~/github/ohi-global/eez2013/scores.csv') %>%
  rename(rgn_id = region_id, lsp_v2015 = score) %>%
  filter(dimension == 'status' & goal == 'LSP') %>%
  left_join(prot_df %>% 
              filter(year == 2012), 
            by = 'rgn_id') %>%
  left_join(rgn_names, by = 'rgn_id')

library(ggplot2)
lsp_plot_v2015 <- ggplot(lsp_v2015, 
                        aes(x = lsp_v2015, y = lsp_status_cells)) +
  geom_point(alpha = .6) +
  theme(legend.position = 'none') +
  geom_abline(slope = 1, intercept = 0, color = 'red') +
  labs(x = 'LSP status v2015 (eez2013: data through 2012)',
       y = 'LSP status v2016 (data through 2012))',
       title = 'LSP status compare')


library(plotly)
ggplotly(lsp_plot_v2015)

ggsave(file.path(dir_goal, 'int/plot_v2015_v2016_cellarea.png'), plot = lsp_plot_v2015)

lsp_v2015 <- lsp_v2015 %>%
  dplyr::select(rgn_id, year, lsp_v2015, lsp_status_rgn, lsp_status_cells, rgn_name) %>%
  mutate(diff = lsp_status_rgn - lsp_v2015 ) %>%
  arrange(diff)

write_csv(lsp_v2015, file.path(dir_goal_anx, 'int/check_vs_2015.csv'))
```

``` {r plot_protected_areas}
library(ggplot2)

x_1km <- prot_1km %>% 
  group_by(year) %>%
  summarize(rgn_id   = 0,
            pct_prot = sum(n_cells_cum) / sum(n_cells_tot)) %>%
  bind_rows(prot_1km) %>%
  filter(year > 1950)

x_3nm <- prot_3nm %>% 
  group_by(year) %>%
  summarize(rgn_id   = 0,
            pct_prot = sum(n_cells_cum) / sum(n_cells_tot)) %>%
  bind_rows(prot_1km) %>%
  filter(year > 1950)

plot_prot_1km <- ggplot(x_1km %>% filter(rgn_id > 0), aes(x = year, y = pct_prot)) +
  geom_line(aes(group = rgn_id, color = rgn_id)) + 
  geom_line(data = x_1km %>% filter(rgn_id == 0),
            aes(group = rgn_id), 
            color = 'red', size = 2, alpha = .7) +
  labs(x = 'year',
       y = 'proportion of region protected',
       title = 'Protected area, 1 km inland',
       color = 'Rgn ID')

print(plot_prot_1km)

plot_prot_3nm <- ggplot(x_3nm %>% filter(rgn_id > 0), aes(x = year, y = pct_prot)) +
  geom_line(aes(group = rgn_id, color = rgn_id)) +
  geom_line(data = x_3nm %>% filter(rgn_id == 0),
            aes(group = rgn_id), 
            color = 'red', size = 2, alpha = .7) +
  labs(x = 'year',
       y = 'proportion of region protected',
       title = 'Protected area, 3 nm offshore',
       color = 'Rgn ID')

print(plot_prot_3nm)


```
[REFERENCE RMD FILE: https://cdn.rawgit.com/OHI-Science/ohiprep/master/globalprep/prs_oa/v2016/oa_dataprep.html]

#Summary
[general description: What data are being generated? Why (what project, etc.)? Upstream/downstream processing information that might be helpful?  Other information?]

#Updates from previous assessment
[Any significant changes in methods from previous analyses?]

***

#Data Source
**Reference**: IUCN and UNEP-WCMC (2016), The World Database on Protected Areas (WDPA) [On-line], May 2016. Cambridge, UK: UNEP-WCMC. Available at: www.protectedplanet.net.

**Downloaded**: June 7, 2016

**Description**:  Shapefile of World Database on Protected Areas

**Time range**: [e.g., 1880-1899, monthly data provided for each year] 

**Format**:  Shapefile

***
  
#Methods
[R code used to generate the data. Or, alternatively, description of the code/data files used to generate the data.]

***

#Citation information  
[citation information: include if these data will have their own specific citation.]